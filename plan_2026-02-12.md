# Steinberger Deep Plan — Nanochat Repo (2026-02-12)

## Scope
- Focus: simplification and architecture coherence after shared-training refactor.
- Files in review: `src/main.rs`, `src/lib.rs`, `src/config.rs`, `src/cli.rs`, `src/experiments/mod.rs`, `src/scalar/mod.rs`, `src/scalar/bigram.rs`, `src/scalar/minigpt.rs`, `src/tensor/mod.rs`, `src/training.rs`.

## File-by-file checks

1. `src/main.rs`
- Validate dispatch ergonomics are now mode-agnostic and avoid error-string conversion at call sites.
- Keep run-or-exit path simple and explicit.

2. `src/lib.rs`
- Confirm module exposure remains minimal and order remains stable.
- No further refactor needed unless we want to hide internal modules behind feature gates.

3. `src/config.rs`
- Keep configuration enums/types stable; only verify derived traits and Display impl completeness for new errors/paths.
- No logic coupling candidates identified.

4. `src/cli.rs`
- Ensure CLI defaults centralize shared config constants through `training::DEFAULT_CHECKPOINT_DIR`.
- Verify parsing surface still matches `Config` usage and test coverage for mini-gpt plus tensor/invalid combos.

5. `src/experiments/mod.rs`
- Keep scalar-only design deliberate.
- Confirm file paths and artifact dir creation are stable after default path move.

6. `src/scalar/mod.rs`
- Central review of mode dispatch (bigram/minigpt), eval/warmdown cadence source, and split helpers delegate cleanly.
- Confirm no duplicated constants remain for corpus/decay logic.

7. `src/scalar/bigram.rs`
- Confirm sampling and transition helpers are fully delegated to `training` and only keep model-specific math.
- Keep Value-graph semantics isolated to bigram-only behavior.

8. `src/scalar/minigpt.rs`
- Confirm duplicated logic moved to `training` where common.
- Verify no residual local `weighted_choice`/pair-split code remains.

9. `src/training.rs` (new)
- Treat as canonical shared layer for corpus styling, sampling helpers, splits, LR schedule.
- Ensure public surface doesn’t accidentally overfit to scalar/tensor internals.

10. `src/tensor/mod.rs`
- Primary simplification payoff: common split/sampling constants and helpers moved to `training`.
- Validate tensor-specific preconditions are explicit (`UnsupportedModelKind`, `UnsupportedOptimizer`).
- Add/keep tests for unsupported combinations and compatibility constraints.

## High-ROI rollout order

1) Validation consistency
- Keep support checks (`model kind`, optimizer capabilities) in all tensor entry paths.

2) Test expansion
- Add mode/optimizer rejection tests in tensor test helper paths for compile-time non-tch fallback.
- Keep parity and regression tests focused on outputs and scheduling.

3) Cross-mode simplification audit
- Confirm all duplicated corpus/split/sampling logic is in `training` and only behavior glue remains in modules.

## Recommendations
- Targeted path is stable: no broad architecture refactor is required.
- Evidence: after migration, tensor/scalar now share same split + sampling constants and test surfaces.

## Execution status
- Added consistency checks in tensor internal test helper.
- Added table-driven coverage for unsupported tensor runtime combos across public API and CLI entrypoints.
- Added non-finite temperature rejection coverage and minimal-dataset split fallback training coverage.
- Implemented Python-parity gating in the parity test to avoid unconditional Python dependency.
- `main.rs` dispatch simplified and made uniform for scalar/tensor output formatting.
- Full verification completed successfully (`cargo test --all`) after installing Rust in this environment.

### Benchmark/status update (Phase 2)
- Executed `scripts/profile_gpu.sh`/`scripts/benchmark_karpathy.py` CPU-only, no-GPU mode with
  minimal run arguments (`ours-steps=8`, `nanochat-num-iterations=1`, `nanochat-max-chars=200`, `--no-require-gpu`).
- Outcomes:
  - Ours tensor run: successful (`mode=tensor`, `using_gpu=false`, `steps=8`, `final_loss=3.295915`, `val_loss=3.295820`).
  - Ours scalar fallback run: successful.
  - `nanochat` baseline run: timed out under configured timeout windows (`120s`, then `180s`).
  - Environment note: `torch` not available in this environment (`No module named 'torch'`).
- Remaining benchmark work:
  - Baseline run remains timeout-bound under current `--nanochat-train-timeout-sec 180` settings on this machine; needs longer budget for full comparison.
  - GPU path can now run successfully from the benchmark harness when `LIBTORCH` points to a local torch tree (the harness currently reports `python_torch: (from LIBTORCH env)` for this setup).
  - Capture comparable GPU timing artifacts when GPU-capable hosts are available.
