# Plan: Clean Stale Files + Next-Level Roadmap

## Context

nanochat-rs-next was fully implemented via Codex in a single development sprint. The codebase is functionally complete for its current scope (62 tests passing, all quality gates green), but carries development artifacts and has clear next steps informed by:

1. **The project's own task tracker** (`docs/tasks.md`) — Phase 1 and Phase 4 are done; Phases 2-3 have open items.
2. **karpathy/nanochat upstream activity** — active PRs on LR schedule improvements (1-sqrt warmdown), optimizer advances (MuonH), dataloader perf (GC elimination, tensor pre-conversion, LRU mask caching), checkpoint-before-eval ordering, and eval memory leaks.
3. **AntigmaLabs/nanochat-rs** — dormant (last commit Dec 2025), no open issues, added a web chat server but no benchmark work.

---

## Part 1: Clean Stale Files

### Delete from disk (not git-tracked, already gitignored)

| Path | Size | Reason |
|------|------|--------|
| `external/nanochat/` | ~1 GB | Cloned upstream; already gitignored; re-cloneable via benchmark script |
| `scripts/__pycache__/` | 52 KB | Python bytecache; should never be committed |
| `references/nanochat/` | 3.3 MB | Local upstream copy; NOT gitignored but only `karpathy_microgpt.py` is tracked |

### Remove from git tracking

| Path | Action |
|------|--------|
| `references/nanochat/` | `rm -rf` the untracked directory (only `references/karpathy_microgpt.py` stays) |
| `docs/plan.md` | Delete — architecture diagram already in CLAUDE.md, phase plan is stale |
| `docs/tasks.md` | Delete — all Phase 1/4/Infra tasks done, remaining Phase 2-3 items captured in this new plan |
| `docs/requirements.md` | Delete — requirements are fulfilled and documented in README |
| `docs/upstream_analysis.md` | **Keep** — this is the only non-redundant doc; has detailed upstream methodology analysis needed for Phase 2 parity work |

### Update .gitignore

Add: `references/nanochat/`, `*.pyc`, `__pycache__/`

### Commands

```bash
rm -rf external/nanochat/
rm -rf scripts/__pycache__/
rm -rf references/nanochat/
rm docs/plan.md docs/tasks.md docs/requirements.md
# Update .gitignore
# Stage: git rm docs/plan.md docs/tasks.md docs/requirements.md
# Commit: "chore: clean stale development artifacts"
```

---

## Part 2: Roadmap — What's Left to Elevate This

### Overview

The project has a solid foundation (scalar autograd, tensor dual-backend, CLI, ablation, checkpointing, 62 tests). To reach the next level, three tracks matter:

| Track | Goal | Difficulty |
|-------|------|------------|
| **A. Benchmark Win** | Demonstrate Rust parity or superiority vs karpathy/nanochat | High |
| **B. Tensor Mini-GPT** | Make `--mode tensor --model-kind mini-gpt` work end-to-end | Medium |
| **C. Quality & Polish** | Fix known issues, modernize, harden | Low-Medium |

### Track A: Benchmark Win (Phase 2 from old tasks.md)

This is the project's thesis — without a demonstrated win, it's just a training exercise.

**A.1: GPU profiling run**
- Run tensor training with `tch-backend` on GPU (Colab T4 or local)
- Collect: steps/sec, tokens/sec, memory usage
- Compare against upstream nanochat at matched budget
- Files: `scripts/profile_gpu.sh` (exists), results in `results/`

**A.2: 1-sqrt warmdown LR schedule** (inspired by upstream PR #513)
- Current: linear warmdown
- Upstream is moving to `LR = 1 - sqrt(progress)` — faster initial decay, longer tail
- Implement as an option in `training.rs`, wire through CLI as `--lr-schedule linear|sqrt`
- Files: `src/training.rs`, `src/cli.rs`, `src/config.rs`

**A.3: BPB (bits-per-byte) validation metric** (from `docs/upstream_analysis.md`)
- Current: mean NLL loss
- Upstream uses BPB: `total_nats / (ln(2) * total_bytes)`
- Needed for apples-to-apples comparison
- Files: `src/training.rs`, `src/scalar/mod.rs`, `src/tensor/mod.rs`

**A.4: Definitive benchmark comparison**
- Run `scripts/benchmark_karpathy.py` on GPU with matched settings
- Produce comparison report
- Update README with findings
- Target: Rust within 5% of Python quality OR faster at matched quality

### Track B: Tensor Mini-GPT (Phase 3 from old tasks.md)

Currently `--mode tensor --model-kind mini-gpt` only works with `tch-backend`. The CPU-native path rejects it. This matters because mini-gpt is where the model gets interesting.

**B.1: CPU-native tensor mini-gpt**
- Port the scalar mini-gpt architecture (multi-head attention + ReGLU MLP) to `Vec<Vec<f64>>` operations
- Or: accept that CPU-native is bigram-only and document this clearly
- Files: `src/tensor/mod.rs`

**B.2: Tensor mini-gpt sampling**
- `sample --mode tensor --model-kind mini-gpt` needs to work
- Files: `src/tensor/mod.rs`

**B.3: Scalar-vs-tensor parity test**
- On the same small input, scalar and tensor mini-gpt should produce loss trajectories within tolerance
- Files: test in `src/tensor/mod.rs` or integration test

### Track C: Quality & Polish

**C.1: Checkpoint-before-eval pattern everywhere** (inspired by upstream issue #446)
- We already implemented this pattern in `checkpoint.rs` — and upstream nanochat has it as an open issue!
- Verify it's wired into ALL train loops (scalar bigram, scalar mini-gpt, tensor)
- This is a differentiator — we solved a problem upstream hasn't yet

**C.2: Eval memory safety** (inspired by upstream issue #427)
- We have `MemoryDriftGuard` — upstream has a memory leak in hellaswag eval
- Wire the guard into actual eval loops (currently only tested in isolation)
- Another differentiator

**C.3: Error handling audit**
- Ensure all error paths are tested
- Check for unwrap() calls that could panic in production
- Files: all `src/*.rs`

**C.4: Integration tests**
- Currently all tests are unit tests
- Add a few end-to-end tests that exercise the CLI binary
- Use `assert_cmd` or similar

**C.5: README benchmark section**
- Even without a GPU win yet, document what CAN be benchmarked today
- Show sample output from a CPU run

### Priority Order

1. **Clean stale files** (Part 1) — immediate, 5 minutes
2. **C.1 + C.2** — wire existing guards into train loops (we already built these, just need to use them)
3. **A.2** — 1-sqrt warmdown (small, high-value, matches upstream direction)
4. **A.3** — BPB metric (needed before any benchmark claim)
5. **B.1/B.2** — tensor mini-gpt (extends capability)
6. **A.1 → A.4** — GPU profiling and benchmark (needs hardware access)

---

## Upstream Intelligence

### karpathy/nanochat — Recent Commits (as of 2025-02-13)

- `2f09686` clarify that this is bf16 mfu we're talking about
- `e569b59` delete torchao dependency, create own Float8Linear (3% faster, simpler)
- `1ec0a34` at depth 28+ need batch size 8
- `ff46300` tune miniseries, keep to even depths for clean model sizing
- `aeff095` better comments on hyperparameter transfer, WD scaling from empirical 1/d^2 to Tepoch-based
- `685271d` new optimal ratio for d26 training
- `5fdd5cd` new leaderboard record via auto-calculated optimal batch size (d26: 1M, up from 0.5M)

### karpathy/nanochat — Key Open Issues

- **#427**: hellaswag eval gets progressively slower and leaks memory on Mac (our `MemoryDriftGuard` addresses this)
- **#446**: checkpoint-before-eval ordering (our `checkpoint.rs` already solves this)
- **#477 (PR)**: Dataloader GC elimination — tensor pre-conversion, LRU mask caching, pool buffer compaction (Rust has no GC — natural advantage)

### AntigmaLabs/nanochat-rs

- Dormant since Dec 2025
- No open issues
- Added web chat server, but no benchmark work against upstream
- Our fork has diverged significantly (autograd engine, mini-gpt, ablation, tensor dual-backend)

---

## Verification

After cleanup:
- `cargo fmt --check` clean
- `cargo clippy --locked -- -D warnings` clean
- `cargo test` — 62/62 passing
- `git status` shows only intentional changes
- No stale files remain outside `src/`, `scripts/`, `docs/upstream_analysis.md`
